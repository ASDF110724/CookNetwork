import os
import json

from fastapi import APIRouter, Body
from dotenv import load_dotenv

from typing import List
from langchain.docstore.document import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from dotenv import load_dotenv

print(load_dotenv())


def load_json_documents(directory: str) -> List[Document]:
    documents = []
    for filename in os.listdir(directory):
        if filename.endswith('.json'):
            file_path = os.path.join(directory, filename)
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
                content = json.dumps(data, ensure_ascii=False, indent=2)
                documents.append(Document(page_content=content, metadata={"source": file_path}))
    return documents


def split_documents(documents: list[Document]) -> list[Document]:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)
    return text_splitter.split_documents(documents)


def create_vector_store(texts):
    # 단어 수준의 임베딩에 적합한 모델 사용
    embeddings = OpenAIEmbeddings()
    vector_store = FAISS.from_documents(texts, embeddings)
    # vector_store = Chroma.from_documents(documents=texts, embedding=embeddings)
    return vector_store


def format_docs(docs):
    return '\n\n'.join(doc.page_content for doc in docs)


# def combine_documents(docs, question):
#     doc_content = format_docs(docs)
#     return {"context": doc_content, "question": question}

def setup_rag(vector_store):
    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={"k": 5})

    """
    당신은 유용한 도우미입니다.아래의 문맥을 사용하여 질문 끝에 있는 질문에 답하세요. 결과는 제목이어야 합니다. 만약 질문의 답을 모른다면, 심지어 그 질문이 한국어로 되어 있더라도, 답을 지어내지 말고 모른다고 말하세요.
    무엇이 들어가냐는 질문에는 아래의 문맥 중 재료에서 포함하는 레시피 제목을 답하세요.
    """
    #
    # template = """당신은 유용한 도우미입니다.아래의 문맥을 사용하여 질문 끝에 있는 질문에 답하세요. 결과는 제목이어야 합니다. 만약 질문의 답을 모른다면, 심지어 그 질문이 한국어로 되어 있더라도, 답을 지어내지 말고 모른다고 말하세요.
    # JSON 구조를 이해하고 관련 정보를 추출하여 답변하세요.
    # 만약, 'aa이 들어가는 bb을 알려줘' 라는 질문에는 문서에서 재료에 aa가 포함된 제목을 답변하세요.
    # 재료에 대한 검색은 정확하게 동일한 단어만 포함합니다.
    # 만약 'aa이 들어가는 bb을 알려줘' 라는 질문에는 aa가 '고추'이고 재료에 '고추장'이 있으면 다른 재료입니다.
    # 만약 'aa이 들어가는 bb을 알려줘' 라는 질문에는 aa가 '고추'이고 재료에 '매운 고추'가 있으면 같은 재료입니다.
    # 답변은 여러 개가 될 수 있지만, 정확한 답변만 할 수 있습니다.
    #
    # 답을 모르면 "주어진 정보로는 답변할 수 없습니다."라고 말하세요.
    #
    # Context: {context}
    #
    # Question: {question}
    # Answer: """
    template = """
    질문에 대한 답을 밑의 내용에서 찾아서 한국어로 정리해서 레시피에 대한 설명으로 답하세요.
    밑의 내용에 정확한 대답이 없으면 "주어진 정보로는 답변할 수 없습니다."라고 말하세요.

    {context}

    질문에 대한 답을 알 수 없다면, "주어진 정보로는 답변할 수 없습니다."라고 말하세요.

    질문: {question}
    대답:"""

    # QA_CHAIN_PROMPT = PromptTemplate(
    #     template=template,
    #     input_variables=["context", "question"]
    # )
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.1)

    # qa_chain = RetrievalQA.from_chain_type(
    #     llm=llm,
    #     chain_type="stuff",
    #     retriever=retriever,
    #     chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
    #     return_source_documents=True,
    # )
    # return qa_chain

    # qa_chain = (
    #     {'context':retriever | format_docs, 'question': RunnablePassthrough()}
    #     | QA_CHAIN_PROMPT
    #     | llm
    #     | StrOutputParser()
    # )

    qa_chain = (
            RunnableParallel(
                {"context": retriever | format_docs, "question": RunnablePassthrough()}
            )
            | QA_CHAIN_PROMPT
            | llm
            | StrOutputParser()
    )

    return qa_chain, retriever


def qa_chain_with_sources(qa_chain, retriever, query):
    result = qa_chain.invoke(query)
    docs = retriever.invoke(query)
    return {"result": result, "source_documents": docs}


def query_rag(qa_chain, query):
    # return qa_chain({"query": query})
    return qa_chain.invoke(query)


# 메인 실행 코드
if __name__ == "__main__":
    directory = "./recipes"

    # texts = load_and_split_documents(directory)
    # print(f"{len(texts)}개의 텍스트 청크를 로드했습니다.")

    # JSON 문서 로드 및 처리
    documents = load_json_documents(directory)
    texts = split_documents(documents)
    print(texts)
    print(f"{len(texts)}개의 텍스트 청크를 생성했습니다.")

    vector_store = create_vector_store(texts)
    print("벡터 저장소 생성 완료")

    qa_chain, retriever = setup_rag(vector_store)
    print("RAG 설정 완료")

    while True:
        query = input("\n요리나 레시피에 대해 질문하세요 (종료하려면 'q' 입력): ")
        if query.lower() == 'q':
            break

        # result = query_rag(qa_chain, query)
        result = qa_chain_with_sources(qa_chain, retriever, query)
        print(result)

        print(f"\n질문: {query}")
        print(f"답변: {result['result']}")

        # print("\n참고한 문서:")
        # for doc in result['source_documents']:
        #     print(f"- 출처: {doc.metadata['source']}")
        #     print(f"- {doc.page_content[:100]}...")  # 첫 100자만 출력

    print("프로그램을 종료합니다.")